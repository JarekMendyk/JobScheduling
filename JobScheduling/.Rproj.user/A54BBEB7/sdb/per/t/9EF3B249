{
    "collab_server" : "",
    "contents" : "```{r}\n###----------------------------------------------------------------------------\n###\n###Required packages: AppliedPredictiveModeling, C50, caret, doMC (optional),\n###                    earth, Hmisc, ipred, tabplot, kernlab, lattice, MASS,\n###                    mda, nnet, pls, randomForest, rpart, sparseLDA, \n###\n### Data used: The HPC job scheduling data in the AppliedPredictiveModeling\n###            package.\n\n\ninstall.packages(\"Hmisc\")\ninstall.packages(\"tabplot\")\nlibrary(Hmisc)\nlibrary(tabplot)\n\n```\n\n```{r}\n################################################################################\n\nlibrary(AppliedPredictiveModeling)\ndata(schedulingData)\n\n### Make a vector of predictor names\npredictors <- names(schedulingData)[!(names(schedulingData) %in% c(\"Class\"))]\n\n### A few summaries and plots of the data\nlibrary(Hmisc)\ndescribe(schedulingData)\n\nlibrary(tabplot)\ntableplot(schedulingData[, c( \"Class\", predictors)])\n\nmosaicplot(table(schedulingData$Protocol, \n                 schedulingData$Class), color = c(2,4,6,7),main = \"Mosaicplot\")\n\nlibrary(lattice)\nxyplot(Compounds ~ InputFields|Protocol,\n       data = schedulingData,\n       scales = list(x = list(log = 10), y = list(log = 10)),\n       groups = Class,\n       xlab = \"Input Fields\",\n       auto.key = list(columns = 4),\n       aspect = 1,\n       as.table = TRUE)\n```\n\n```{r}\n#Data Splitting and Model Strategy\n\n## Split the data\n\nlibrary(caret)\nset.seed(1104)\ninTrain <- createDataPartition(schedulingData$Class, p = .8, list = FALSE)\n\n### There are a lot of zeros and the distribution is skewed. We add\n### one so that we can log transform the data\nschedulingData$NumPending <- schedulingData$NumPending + 1\n\ntrainData <- schedulingData[ inTrain,]\ntestData  <- schedulingData[-inTrain,]\n\n### Create a main effects only model formula to use\n### repeatedly. Another formula with nonlinear effects is created\n### below.\nmodForm <- as.formula(Class ~ Protocol + log10(Compounds) +\n  log10(InputFields)+ log10(Iterations) +\n  log10(NumPending) + Hour + Day)\n\n### Create an expanded set of predictors with interactions. \n\nmodForm2 <- as.formula(Class ~ (Protocol + log10(Compounds) +\n  log10(InputFields)+ log10(Iterations) +\n  log10(NumPending) + Hour + Day)^2)\n\n\n### Some of these terms will not be estimable. For example, if there\n### are no data points were a particular protocol was run on a\n### particular day, the full interaction cannot be computed. We use\n### model.matrix() to create the whole set of predictor columns, then\n### remove those that are zero variance\n\nexpandedTrain <- model.matrix(modForm2, data = trainData)\nexpandedTest  <- model.matrix(modForm2, data = testData)\nexpandedTrain <- as.data.frame(expandedTrain)\nexpandedTest  <-  as.data.frame(expandedTest)\n\n### Some models have issues when there is a zero variance predictor\n### within the data of a particular class, so we used caret's\n### checkConditionalX() function to find the offending columns and\n### remove them\n\nzv <- checkConditionalX(expandedTrain, trainData$Class)\n\n### Keep the expanded set to use for models where we must manually add\n### more complex terms (such as logistic regression)\n\nexpandedTrain <-  expandedTrain[,-zv]\nexpandedTest  <-  expandedTest[, -zv]\n\n```\n\n```{r}\n\n### Create the cost matrix\ncostMatrix <- ifelse(diag(4) == 1, 0, 1)\ncostMatrix[4, 1] <- 10\ncostMatrix[3, 1] <- 5\ncostMatrix[4, 2] <- 5\ncostMatrix[3, 2] <- 5\nrownames(costMatrix) <- colnames(costMatrix) <- levels(trainData$Class)\n\n### Create a cost function\ncost <- function(pred, obs)\n{\n  isNA <- is.na(pred)\n  if(!all(isNA))\n  {\n    pred <- pred[!isNA]\n    obs <- obs[!isNA]\n    \n    cost <- ifelse(pred == obs, 0, 1)\n    if(any(pred == \"VF\" & obs == \"L\")) cost[pred == \"L\" & obs == \"VF\"] <- 10\n    if(any(pred == \"F\" & obs == \"L\")) cost[pred == \"F\" & obs == \"L\"] <- 5\n    if(any(pred == \"F\" & obs == \"M\")) cost[pred == \"F\" & obs == \"M\"] <- 5\n    if(any(pred == \"VF\" & obs == \"M\")) cost[pred == \"VF\" & obs == \"M\"] <- 5\n    out <- mean(cost)\n  } else out <- NA\n  out\n}\n\n### Make a summary function that can be used with caret's train() function\ncostSummary <- function (data, lev = NULL, model = NULL)\n{\n  if (is.character(data$obs))  data$obs <- factor(data$obs, levels = lev)\n  c(postResample(data[, \"pred\"], data[, \"obs\"]),\n    Cost = cost(data[, \"pred\"], data[, \"obs\"]))\n}\n\n### Create a control object for the models\nctrl <- trainControl(method = \"repeatedcv\", \n                     repeats = 5,\n                     summaryFunction = costSummary)\n```\n\n```{r}\n### Optional: parallel processing can be used via the 'do' packages,\n### such as doMC, doMPI etc. We used doMC (not on Windows) to speed\n### up the computations.\n\n### WARNING: Be aware of how much memory is needed to parallel\n### process. It can very quickly overwhelm the available hardware. The\n### estimate of the median memory usage (VSIZE = total memory size) \n### was 3300-4100M per core although the some calculations require as  \n### much as 3400M without parallel processing. \n\nlibrary(doMC)\nregisterDoMC(14)\n\n### Fit the CART model with and without costs\n\nset.seed(857)\nrpFit <- train(x = trainData[, predictors],\n               y = trainData$Class,\n               method = \"rpart\",\n               metric = \"Cost\",\n               maximize = FALSE,\n               tuneLength = 20,\n               trControl = ctrl)\nrpFit\n\nset.seed(857)\nrpFitCost <- train(x = trainData[, predictors],\n                   y = trainData$Class,\n                   method = \"rpart\",\n                   metric = \"Cost\",\n                   maximize = FALSE,\n                   tuneLength = 20,\n                   parms =list(loss = costMatrix),\n                   trControl = ctrl)\nrpFitCost\n\nset.seed(857)\nldaFit <- train(x = expandedTrain,\n                y = trainData$Class,\n                method = \"lda\",\n                metric = \"Cost\",\n                maximize = FALSE,\n                trControl = ctrl)\nldaFit\n\nsldaGrid <- expand.grid(NumVars = seq(2, 112, by = 5),\n                        lambda = c(0, 0.01, .1, 1, 10))\nset.seed(857)\nsldaFit <- train(x = expandedTrain,\n                 y = trainData$Class,\n                 method = \"sparseLDA\",\n                 tuneGrid = sldaGrid,\n                 preProc = c(\"center\", \"scale\"),\n                 metric = \"Cost\",\n                 maximize = FALSE,\n                 trControl = ctrl)\nsldaFit\n\nset.seed(857)\nnnetGrid <- expand.grid(decay = c(0, 0.001, 0.01, .1, .5),\n                        size = (1:10)*2 - 1)\nnnetFit <- train(modForm, \n                 data = trainData,\n                 method = \"nnet\",\n                 metric = \"Cost\",\n                 maximize = FALSE,\n                 tuneGrid = nnetGrid,\n                 trace = FALSE,\n                 MaxNWts = 2000,\n                 maxit = 1000,\n                 preProc = c(\"center\", \"scale\"),\n                 trControl = ctrl)\nnnetFit\n\nset.seed(857)\nplsFit <- train(x = expandedTrain,\n                y = trainData$Class,\n                method = \"pls\",\n                metric = \"Cost\",\n                maximize = FALSE,\n                tuneLength = 100,\n                preProc = c(\"center\", \"scale\"),\n                trControl = ctrl)\nplsFit\n\nset.seed(857)\nfdaFit <- train(modForm, data = trainData,\n                method = \"fda\",\n                metric = \"Cost\",\n                maximize = FALSE,\n                tuneLength = 25,\n                trControl = ctrl)\nfdaFit\n\nset.seed(857)\nrfFit <- train(x = trainData[, predictors],\n               y = trainData$Class,\n               method = \"rf\",\n               metric = \"Cost\",\n               maximize = FALSE,\n               tuneLength = 10,\n               ntree = 2000,\n               importance = TRUE,\n               trControl = ctrl)\nrfFit\n\nset.seed(857)\nrfFitCost <- train(x = trainData[, predictors],\n                   y = trainData$Class,\n                   method = \"rf\",\n                   metric = \"Cost\",\n                   maximize = FALSE,\n                   tuneLength = 10,\n                   ntree = 2000,\n                   classwt = c(VF = 1, F = 1, M = 5, L = 10),\n                   importance = TRUE,\n                   trControl = ctrl)\nrfFitCost\n\nc5Grid <- expand.grid(trials = c(1, (1:10)*10),\n                      model = \"tree\",\n                      winnow = c(TRUE, FALSE))\nset.seed(857)\nc50Fit <- train(x = trainData[, predictors],\n                y = trainData$Class,\n                method = \"C5.0\",\n                metric = \"Cost\",\n                maximize = FALSE,\n                tuneGrid = c5Grid,\n                trControl = ctrl)\nc50Fit\n\nset.seed(857)\nc50Cost <- train(x = trainData[, predictors],\n                 y = trainData$Class,\n                 method = \"C5.0\",\n                 metric = \"Cost\",\n                 maximize = FALSE,\n                 costs = costMatrix,\n                 tuneGrid = c5Grid,\n                 trControl = ctrl)\nc50Cost\n\nset.seed(857)\nbagFit <- train(x = trainData[, predictors],\n                y = trainData$Class,\n                method = \"treebag\",\n                metric = \"Cost\",\n                maximize = FALSE,\n                nbagg = 50,\n                trControl = ctrl)\nbagFit\n\n### Use the caret bag() function to bag the cost-sensitive CART model\nrpCost <- function(x, y)\n{\n  costMatrix <- ifelse(diag(4) == 1, 0, 1)\n  costMatrix[4, 1] <- 10\n  costMatrix[3, 1] <- 5\n  costMatrix[4, 2] <- 5\n  costMatrix[3, 2] <- 5\n  library(rpart)\n  tmp <- x\n  tmp$y <- y\n  rpart(y~., data = tmp, control = rpart.control(cp = 0),\n        parms =list(loss = costMatrix))\n}\nrpPredict <- function(object, x) predict(object, x)\n\nrpAgg <- function (x, type = \"class\")\n{\n  pooled <- x[[1]] * NA\n  n <- nrow(pooled)\n  classes <- colnames(pooled)\n  for (i in 1:ncol(pooled))\n  {\n    tmp <- lapply(x, function(y, col) y[, col], col = i)\n    tmp <- do.call(\"rbind\", tmp)\n    pooled[, i] <- apply(tmp, 2, median)\n  }\n  pooled <- apply(pooled, 1, function(x) x/sum(x))\n  if (n != nrow(pooled)) pooled <- t(pooled)\n  out <- factor(classes[apply(pooled, 1, which.max)], levels = classes)\n  out\n}\n\n\nset.seed(857)\nrpCostBag <- train(trainData[, predictors],\n                   trainData$Class,\n                   \"bag\",\n                   B = 50,\n                   bagControl = bagControl(fit = rpCost,\n                                           predict = rpPredict,\n                                           aggregate = rpAgg,\n                                           downSample = FALSE,\n                                           allowParallel = FALSE),\n                   trControl = ctrl)\nrpCostBag\n\nset.seed(857)\nsvmRFit <- train(modForm ,\n                 data = trainData,\n                 method = \"svmRadial\",\n                 metric = \"Cost\",\n                 maximize = FALSE,\n                 preProc = c(\"center\", \"scale\"),\n                 tuneLength = 15,\n                 trControl = ctrl)\nsvmRFit\n\nset.seed(857)\nsvmRFitCost <- train(modForm, data = trainData,\n                     method = \"svmRadial\",\n                     metric = \"Cost\",\n                     maximize = FALSE,\n                     preProc = c(\"center\", \"scale\"),\n                     class.weights = c(VF = 1, F = 1, M = 5, L = 10),\n                     tuneLength = 15,\n                     trControl = ctrl)\nsvmRFitCost\n\nmodelList <- list(C5.0 = c50Fit,\n                  \"C5.0 (Costs)\" = c50Cost,\n                  CART =rpFit,\n                  \"CART (Costs)\" = rpFitCost,\n                  \"Bagging (Costs)\" = rpCostBag,\n                  FDA = fdaFit,\n                  SVM = svmRFit,\n                  \"SVM (Weights)\" = svmRFitCost,\n                  PLS = plsFit,\n                  \"Random Forests\" = rfFit,\n                  LDA = ldaFit,\n                  \"LDA (Sparse)\" = sldaFit,\n                  \"Neural Networks\" = nnetFit,\n                  Bagging = bagFit)\n```\n\n```{r}\nResults\n\nrs <- resamples(modelList)\nsummary(rs)\n\nconfusionMatrix(rpFitCost, \"none\")\nconfusionMatrix(rfFit, \"none\") \n\nplot(bwplot(rs, metric = \"Cost\"))\n\nrfPred <- predict(rfFit, testData)\nrpPred <- predict(rpFitCost, testData)\n\nconfusionMatrix(rfPred, testData$Class)\nconfusionMatrix(rpPred, testData$Class)\n\n```\n\n",
    "created" : 1507037401327.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "755396731",
    "id" : "9EF3B249",
    "lastKnownWriteTime" : 1507108630,
    "last_content_update" : 1507108630296,
    "path" : "~/GitHub/JobScheduling/JobScheduling/job_scheduling_notes.Rmd",
    "project_path" : "job_scheduling_notes.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}